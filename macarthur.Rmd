---
title: "Scraping Data: Example"
author:
- name: "Kieran Healy"
  affiliation: "Duke University"
  email: "kjhealy@soc.duke.edu"
date: "March 18, 2020"
output:
  html_document: distill::distill_article
  pdf_document:
    template: ~/.pandoc/templates/rmd-latex.template
---

# Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.showtext=TRUE)
```

```{r libraries, message = FALSE}
library(tidyverse)
library(rvest)
library(socviz)
```

```{r theme, message = FALSE}
library(showtext)
showtext_auto()
library(myriad)
import_myriad_semi()

theme_set(theme_myriad_semi())
```

# An inital web-scraping example, following Brad Boehmke

When grabbing data from the web, we ideally like to get it via a clean API, so we know just how to talk to the server providing us with the data, and just how to work with what it sends us. This also has the advantage of working with the server in a way that it (and its owners) expect. Sometimes this is isn't possible, and we end up _scraping_ the website instead. In comparison to interacting with a decent API, scraping is messy, fragile, and generally gross. 

Note also that you should in general be _very careful_ when it comes to scraping even modest amounts of data from websites, because R can send requests to servers much faster than you can do manually. If you are rude to servers by repeatedly hammering their pages, you may well get throttled or banned from them. Note that, also, systematically scraping data from a website is often _explicitly forbidden_ by the website's owners. Don't do it blithely, or more often than you have to. 

Scraping elements of a web page means grabbing the source HTML of the page, isolating the pieces you want to get (say, a table), and munging them into a tibble. The basic tidyverse tool in R for this is the `rvest` library. There are a number of tutorials and walkthroughs for using `rvest` and its functions. For example [http://bradleyboehmke.github.io//2015/12/scraping-html-text.html](Brad Boehmke)'s introduction, which we'll follow here for the first part. 

First we're going to look at a webpage from the Bureau of Labor Statistics: [http://www.bls.gov/web/empsit/cesbmart.htm]

We grab its content with `read_html()`:

```{r }
bls <- read_html("http://www.bls.gov/web/empsit/cesbmart.htm")

bls
```

That gets us the raw content of the file. Now we need to pick through it and get the table we want. We can reach in and extract html _nodes_ (i.e. structural components of the document) matching various descriptions: 

```{r}
bls_tabs <- html_nodes(bls, "table")

bls_tabs
```

There are a bunch of tables on the page. Let's get Table 9, "Net birth-death forecasts by industry supersector, April to December 2019". 

```{r}
bls_tbl9 <- bls %>%
  html_nodes("#Table9") %>%
  html_table(fill = TRUE) 
  
bls_tbl9
```

I told you scraping was gross. We need to get rid of the last three rows of the table, containing a blank line, the cumulative totals, and the footnote row. 

```{r}
bls_tbl9[[1]][1:14,] ## bls_tbl9 is a list
```

```{r}
bls_tbl9 <- bls_tbl9[[1]][1:14,] %>%
  as_tibble()

bls_tbl9
```

Now we can clean the column names and tidy dates. We also drop the Cumultative Total column.

```{r}
bls_tbl9 %>%
  janitor::clean_names() %>%
  select(-cumulative_total) %>%
  pivot_longer(apr:dec, names_to = "month", values_to = "forecast") %>%
  mutate(month = snakecase::to_title_case(month), 
         forecast = as.numeric(forecast))
```

Now we have a table we can work with.


# MacArthur Fellows Example

Here's a more involved example. We're going to scrape a lot of pages and extract non-table elements from them.

### Scrape the Website

Get a List of all MacArthur Fellows from `macfound.org` and put them in a list. Don't do this more than once! This code is not evaluated when knitting the document, because we only have to do it once. When doing this yourself ss an excersie, please just choose, say 25 pages at random, with e.g. `sample(1:054, 25)` in place of `1:1054` in the line of code that makes the `urls` vector. 

```{r, eval = FALSE, echo = TRUE}

### Generate vector of fellow page urls
urls <- paste0("https://www.macfound.org/fellows/", 1:1054, "/")


### Grab the full Macfound page of every awardee from macfound.org
bio_pages <- urls %>% 
  map(~ {
    message(glue::glue("* parsing: {.x}"))
    Sys.sleep(sample(rpois(n = 1, lambda = 8))) # try to be polite
    safely(read_html)(.x)
  })

```

### Save the scraped webpages locally

There's a gotcha with objects like `bio_pages`: they cannot be straightforwardly saved to R's native data format with `save()`. The XML files are stored with external pointers to their content and cannot be "serialized" in a way that saves their content properly. If you try, when you `load()` the saved object you will get complaints about missing pointers. So instead, we'll unspool our list and save each fellow's page individually. Then if we want to rerun this analysis without crawling everything again, we will load them in from our local saved versions using `read_html()`.

Again, this code chunk is shown but not run, as we only do it once. 

```{r localsave, eval = FALSE, echo = TRUE}

### Get a list containing every fellow's webpage, 
### Drop the safely() error codes from the initial scrape, and 
### and also drop any NULL entries
page_list <- pluck(bio_pages, "result") %>% 
  compact()

### Make a vector of clean file names of the form "raw/macfound.org/jane_doe.html"
### One for every fellow. Same order as the page_list.
fnames <-paste0("raw/macfound.org/", 
                janitor::make_clean_names(fellows$name),
                ".html") 
names(fnames) <- fellows$name   

### Walk the elements of the page list and the file names to 
### save each HTML file under is respective clean file name
walk2(page_list, fnames, ~ write_xml(.x, file = .y))

```

# Parse the pages 

Using the local data we've saved, we read in a list of all the MacArthur Fellow web pages.

```{r localparse}

### The filenames we just created
local_urls <- fs::dir_ls("raw/macfound.org/")

### Grab the full Macfound page of every awardee. 
bio_pages <- local_urls %>% 
  map(~ {
    safely(read_html)(.x)
  })

### quick look at first five items in the list
summary(bio_pages)[1:5,]

### Quick look inside the first record
bio_pages[[1]]

```

Next, we parse every webpage to create a record for each fellow. Here's the function that will do the work:

```{r}
get_fellow <- function(x) {
    ifelse(is.null(x), return(NA), FALSE)
    
    f_name <- rvest::html_node(x, ".has-top-margin") %>%
          rvest::html_text(trim = TRUE)

    year_regex <- paste0("Class of( January| February| March| April|",
                          " May| June| July| August| September| October|", 
                          " November| December)?\\s+\\d{4}")
    
    f_year <- rvest::html_node(x, "h2") %>%
          rvest::html_text(trim = TRUE) %>%
          stringr::str_extract(year_regex) %>%
          stringr::str_extract("\\d{4}")
  
    out <- rvest::html_node(x, ".photo-bio__content") %>%
          rvest::html_text(trim = TRUE) %>%
          stringr::str_remove_all("\n              ")
        
    f_title <- str_trim(str_match(out, "Title (.*?) Affiliation")[,2])
    f_affil <- str_trim(str_match(out, "Affiliation (.*?) Location")[,2])
    f_loc <- str_trim(str_match(out, "Location (.*?) Age")[,2])
    f_age <- str_trim(str_extract(out, "Age(.*?)\\d{2}")) %>% 
      str_extract("\\d{2}")
    f_area <- str_trim(str_match(out, "Area of Focus (.*?)$")[,2])

    f_bio <- rvest::html_node(x, ":nth-child(3) .text-content--small") %>%
            rvest::html_text()     
    
    record <- tibble(
      name = f_name,
      year = f_year,
      title = f_title,
      affiliation = f_affil,
      location = f_loc,
      age = f_age,
      area = f_area,
      bio = f_bio)
    
    record
  }

```

Now we apply it to the list of pages:

```{r}

fellows <-  bio_pages %>% 
  pluck("result") %>% # Get the webpages
  compact() %>% # Drop any empties
  map(get_fellow) %>% # Generate the individual records
  bind_rows() # and combine

fellows

```

# Cleaning the data

We write a function to guess the gender of the fellow from their bio information. 

```{r}

infer_gender <- function(bio){
  
  m_regex <- paste0("(\\b[Hh]e is)|(\\b[Hh]e was)|\\b[H]is\\b|",
                    "\\b([Hh]e joined)|(\\b[Hh]e has)|",
                    "\\b[Hh]e explore|\\b[Hh]e also|\\b[Hh]e produced|",
                    "\\b[Hh]e founded|","\\b[Hh]e also|\\b[Hh]e worked|",
                    "\\b[Hh]is work|\\b[Hh]is research")
  
  f_regex <- paste0("(\\b[Ss]he is)|(\\b[Ss]he was)|\\b[H]er\\b|",
                    "\\b([Ss]he joined)|(\\b[Ss]he has)|",
                    "\\b[Ss]he explore|\\b[Ss]he also|\\b[Ss]he produced|",
                    "\\b[Ss]he founded|","\\b[Ss]he worked|",
                    "\\b[Hh]er work|\\b[Hh]er research")
  
  m_test <- str_detect(bio, m_regex)
  
  
  f_test <- str_detect(bio, f_regex)
  
  out <- case_when(
    m_test == TRUE & f_test == FALSE ~ "Male", 
    m_test == FALSE & f_test == TRUE ~ "Female",
    TRUE ~ NA_character_
    )
  out
}
```

We apply it to our data, also making the (character) records of age and fellowship year into integers and dates, respectively:

```{r}
fellows <- fellows %>%
  mutate(age = as.integer(age),
         year = int_to_year(as.integer(year)),
         sex = infer_gender(bio)) %>%
  select(name, year, age, sex, title, everything())

fellows
```

### Manually recoding records 

```{r}
### Eiko and Koma Otake
koma <- fellows %>% filter(name == "Eiko and Koma Otake")
koma$name <- "Koma Otake"

eiko <- fellows %>% filter(name == "Eiko and Koma Otake")
eiko$name <- "Eiko Otake"

fellows <- fellows %>% filter(name %nin% "Eiko and Koma Otake")
fellows <- rbind(fellows, eiko, koma)

### Manually code the remaining gender vals
fellows %>% filter(is.na(sex)) %>% select(name) %>% data.frame()

fellows <- fellows %>%
  mutate(sex = case_when(
  name == "Lin He" ~ as.character("Female"),
  name == "Aaron Shirley" ~ as.character("Male"),
  name == "Michael Lerner" ~ as.character("Male"),
  name == "Edward V. Roberts" ~ as.character("Male"),
  name == "Aaron Lansky" ~ as.character("Male"),
  name == "Patricia Locke" ~ as.character("Female"),
  name == "Robert H. McCabe" ~ as.character("Male"),
  name == "Jeraldyne Blunden" ~ as.character("Female"),
  name == "Martin Daniel Eakes" ~ as.character("Male"),
  name == "Louis Massiah" ~ as.character("Male"),
  name == "Eiko Otake" ~ as.character("Female"),
  name == "Koma Otake" ~ as.character("Male"),
  name == "Kun-Liang Guan" ~ as.character("Male"),
  name == "William W. McDonald" ~ as.character("Male"),
  name == "Juan Martin Maldacena" ~ as.character("Male"),
  name == "Ken Vandermark" ~ as.character("Male"),
  name == "Lucy Blake" ~ as.character("Female"),
  name == "Katherine Gottlieb" ~ as.character("Female"),
  name == "Nancy Siraisi" ~ as.character("Female"),
  name == "Whitfield Lovell" ~ as.character("Male"),
  name == "Junot DÃ­az" ~ as.character("Male"),
  name == "Jeffrey Brenner" ~ as.character("Male"),
  name == "Michelle Dorrance" ~ as.character("Female"),
  name == "Nicole Eisenman" ~ as.character("Female"),
  name == "LaToya Ruby Frazier" ~ as.character("Female"),
  name == "Taylor Mac" ~ as.character("Male"),
  #name == "Wu Tsang" ~ as.character(), ### one nonbinary case
  name == "Cameron Rowland" ~ as.character("Male"),
  TRUE ~ sex))

```

### Clean the `area` field a little

```{r}

clean_area <- function(x) {
  stringr::str_remove(x, "(Website|Twitter).*$") %>%
  stringr::str_squish()
}

fellows <- fellows %>%
  mutate(area = clean_area(area))

```

### Clean Affiliation and Title

```{r}

affil_regex <- paste0("(School of.*?,)|(College of.*?,)|",
                      "(Division of.*?,)|(Department of.*?,)|",
                      "(, .*?Department)|(Center.*?,)")

fellows <- fellows %>%
  mutate(affiliation = replace_na(affiliation, "None Provided"), 
         title = replace_na(title, "None Provided"),
         area = replace_na(area, "None Provided"),
         affil_short = str_trim(str_remove_all(affiliation, affil_regex)),
         title_short = str_trim(str_remove_all(title, ",.*$")),
         area_short = str_trim(str_remove_all(area, ",.*$")))

fellows

```


### Write out the cleaned data to a CSV file

```{r}
write_csv(fellows, "data/macarthur-mfd.csv")

### Partital selection for class problem set:
### fellows %>% select(name, year, age, sex) %>%
### write_csv("~/Documents/courses/visualizingsociety.com/static/data/macarthur.csv")

```




# Make some plots

### Age Distributions

```{r ages, fig.width = 12, layout="l-screen-inset"}

p <- ggplot(data = fellows, 
              mapping = aes(x = year, y = age, group = year))

p + geom_jitter(alpha = 0.5, shape = 1, position = position_jitter()) +  
  geom_boxplot(outlier.size = 0, 
                   fill = my.colors("bly")[2], 
                   alpha = 0.2) + 
  theme(legend.position = "top") +
  labs(x = "Year of Award", y = "Age at Time of Award", 
       title = "Age Distribution of MacArthur Award Winners 1981-2019", 
       caption = "Kieran Healy @kjhealy socviz.co / Data: MacArthur Foundation.") + 
  scale_x_date(breaks = c(int_to_year(1981), 
                          int_to_year(1991), 
                          int_to_year(2001), 
                          int_to_year(2011), 
                          int_to_year(2019)), 
               labels = c("1981", "1991", "2001", "2011", "2019")) + 
  theme(axis.text = element_text(size = rel(1.25)),
        axis.title = element_text(size = rel(1.25)), 
        plot.title = element_text(size = rel(1.75)),
        plot.caption = element_text(size = rel(1.1)))
```

```{r agesex, fig.width = 12, layout="l-screen-inset"}
fellows %>%
  group_by(sex) %>%
  tally() %>%
 mutate(pct = (n / sum(n))*100) %>%
  ggplot(data = subset(fellows, !is.na(sex)), 
              mapping = aes(x = year, y = age, color = sex, fill = sex))+
  geom_boxplot(outlier.colour = NA, 
                   mapping = aes(group = interaction(year, sex), 
                                 color = sex,
                                 fill = sex), alpha = 0.3) + 
  geom_jitter(alpha = 0.4, shape = 1, 
              position = position_jitter(height = 0.2)) + 
  theme(legend.position = "top") +
  scale_fill_manual(values = my.colors("bly")) + 
  scale_color_manual(values = my.colors("bly")) +
  labs(x = "Year of Award", y = "Age at Time of Award", 
       color = "Gender", fill = "Gender",
       title = "Age Distribution of MacArthur Winners over time")
```

### Affiliation, Title, Area, and Location

```{r dotplot}

### A convenience function to make the same kind of dotplot repeatedly
mac_dotplot <- function(data, xvar = pct, yvar = n_lab, 
      x = NULL, y = NULL, title = NULL, 
      subtitle = NULL, 
      caption = "Kieran Healy @kjhealy / Data: MacArthur Foundation.") {
  ggplot(data = data, 
         mapping = aes(x = {{ xvar }},
                       y = reorder({{ yvar }}, {{ xvar }}))) + 
    geom_point(size = 3) + 
    labs(x = x, 
         y = y, 
         title = title,
         subtitle = subtitle,
         caption = caption) +
     theme(axis.text = element_text(size = rel(1.25)),
        axis.title = element_text(size = rel(1.25)), 
        plot.title = element_text(size = rel(1.75)),
        plot.caption = element_text(size = rel(1.1)))  
}

```

#### Affiliation

```{r affil-all, fig.height=12, fig.width=10}
fellows %>% 
  group_by(affil_short) %>%
  tally() %>%
  mutate(freq = n / sum(n),
         pct = round((freq*100), 1),
         n_lab = paste0(affil_short," (", n, ")")) %>%
  arrange(desc(pct)) %>% 
  filter(n > 2) %>%
  select(affil_short, n_lab, pct) %>%
  mac_dotplot(x = "Percent of All Awards", 
              title = "MacArthur Fellowships by Affiliation, 1981-2019", 
              subtitle = "Institutions with three or more affiliated fellows") 
```

```{r affil-nona, fig.height=12, fig.width=10}
fellows %>% 
  group_by(affil_short) %>%
  tally() %>%
  mutate(freq = n / sum(n),
         pct = round((freq*100), 1),
         n_lab = paste0(affil_short," (", n, ")")) %>%
  filter(affil_short %nin% "None Provided") %>%
  arrange(desc(pct)) %>% 
  filter(n > 2) %>%
  select(affil_short, n_lab, pct) %>%
  mac_dotplot(x = "Percent of All Awards", 
              title = "MacArthur Fellowships by Affiliation, 1981-2019", 
              subtitle = "Institutions with three or more affiliated fellows. 
              Excludes 'No Affiliation.'") 
```


#### Title

```{r titles, fig.height=12, fig.width=10}
fellows %>% 
  group_by(title_short) %>%
  tally() %>%
  mutate(freq = n / sum(n),
         pct = round((freq*100), 1),
         n_lab = paste0(title_short," (", n, ")")) %>%
  arrange(desc(pct)) %>% 
  filter(n > 2) %>%
  select(n_lab, pct) %>%
  mac_dotplot(x = "Percent of All Awards", 
       title = "MacArthur Fellowships by Title, 1981-2019", 
       subtitle = "Titles mentioned three or more times")
```

```{r titles-nona, fig.height=12, fig.width=10}
fellows %>% 
  group_by(title_short) %>%
  tally() %>%
  mutate(freq = n / sum(n),
         pct = round((freq*100), 1),
         n_lab = paste0(title_short," (", n, ")")) %>%
  filter(title_short %nin% "None Provided") %>%
  arrange(desc(pct)) %>% 
  filter(n > 2) %>%
  select(n_lab, pct) %>%
  mac_dotplot(x = "Percent of All Awards", 
       title = "MacArthur Fellowships by Title, 1981-2019", 
       subtitle = "Titles mentioned three or more times. 
       Excludes 'No Title.'")
```

#### Area

```{r areas, fig.height=12, fig.width=10}
fellows %>% 
  group_by(area_short) %>%
  tally() %>%
  mutate(freq = n / sum(n),
         pct = round((freq*100), 1), 
         n_lab = paste0(area_short," (", n, ")")) %>%
  arrange(desc(pct)) %>% 
  filter(pct >=1) %>%
  select(n_lab, pct) %>%
  mac_dotplot(x = "Percent of All Awards", 
              title = "MacArthur Fellowships by Area, 1981-2019", 
              subtitle = "Areas with one percent or more of all awards")
```

#### Locations

```{r locations, fig.height=12, fig.width=10}

fellows %>% 
  group_by(location) %>%
  tally() %>%
  mutate(freq = n / sum(n),
         pct = round((freq*100), 1),
         n_lab = paste0(location," (", n, ")")) %>%
  arrange(desc(pct)) %>% 
  filter(pct >= 1) %>%
  select(n_lab, pct) %>%
  ggplot(mapping = aes(x = pct, y = reorder(n_lab, pct))) + 
  geom_point(size = 3) + 
  labs(x = "Percent of All Awards", 
       y = NULL, 
       title = "MacArthur Fellowships by Location, 1981-2019", 
       subtitle = "Locations with one percent or more of all awards") +
  theme(axis.text = element_text(size = rel(1.25)),
        axis.title = element_text(size = rel(1.25)), 
        plot.title = element_text(size = rel(1.75)),
        plot.caption = element_text(size = rel(1.1)))  

```

### Session Information

```{r}
sessioninfo::session_info()
```

